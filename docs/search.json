[
  {
    "objectID": "surveys/index.html",
    "href": "surveys/index.html",
    "title": "TidyTuesday: Retail Sales data analysis with Plotly in R",
    "section": "",
    "text": "In this post, I will analyze the TidyTuesday dataset about Retail Sales …"
  },
  {
    "objectID": "datafest/index.html",
    "href": "datafest/index.html",
    "title": "TidyTuesday: Retail Sales data analysis with Plotly in R",
    "section": "",
    "text": "In this post, I will analyze the TidyTuesday dataset about Retail Sales I am curious how much I can type before ashfojha kajf akjdsfkjdf third line"
  },
  {
    "objectID": "datafest.html",
    "href": "datafest.html",
    "title": "Consulting",
    "section": "",
    "text": "TidyTuesday: Retail Sales data analysis with Plotly in R\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nvisualization\n\n\nplotly\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\nNathan Bresette\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html",
    "href": "posts/NCAA Basketball/basketball.html",
    "title": "NCAA Basketball Analysis",
    "section": "",
    "text": "Performed web scraping using Selenium and BeautifulSoup, followed by an in-depth analysis in that included Principal Component Analysis, XGBoost, and neural networks."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#abstract",
    "href": "posts/NCAA Basketball/basketball.html#abstract",
    "title": "NCAA Basketball Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project presents an analysis of college basketball team performance based on data from men’s NCAA Basketball. After merging the datasets from haslametrics and teamrankings along with data cleaning and feature engineering in R, the dataset consisted of 16 columns with 361 rows.\nExploratory data analysis includes correlation analysis, visualization of distributions, and principal component analysis (PCA) to address collinearity among variables. Although PCA had good insights, it was not utilized due to its limited account for variance.\nThe feature engineered variable, ‘Rank_Category’, classifies teams into three categories based on their ‘Rank’ column: Rank (0-25), Top 50% (excluding Rank), and Bottom 50%. Modeling efforts focused on predicting ‘Rank_Category’ using XGBoost with racing ANOVA tuning which resulted in an accuracy of 79.12% and an AUC of 0.918. Variable importance analysis showed key predictors including defensive efficiency, win rate, defensive field goal percentage, offensive 2-point percentage, and offensive turnovers. Additionally, a neural network model achieved a higher accuracy of 97.80%."
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "href": "posts/NCAA Basketball/basketball.html#webscraping-data-in-python",
    "title": "NCAA Basketball Analysis",
    "section": "Webscraping Data in Python",
    "text": "Webscraping Data in Python\nScraping the first website Halsametrics.com with selenium\n\nCodefrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\nimport requests\nfrom bs4 import BeautifulSoup\nfrom io import StringIO\n\n\n# Set up the WebDriver with ChromeOptions\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('/usr/local/bin/chromedriver')  # Add the path to chromedriver executable\n\n# Initialize the WebDriver\ndriver = webdriver.Chrome(options=chrome_options)\n\n# Navigate to the webpage\ndriver.get('https://haslametrics.com/')\n\n# Wait for the page to load and for the 'Defense' button to be clickable\nwait = WebDriverWait(driver, 20)\ndefense_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//*[@id=\"cboRatings\"]/option[@value=\"Defense\"]')))\n\n# Click the 'Defense' button to load the defensive ratings\ndefense_button.click()\n\n# Wait for the table to load\nwait.until(EC.presence_of_element_located((By.XPATH, '//*[@id=\"myTable\"]')))\n\n# Scrape the table\ntable = driver.find_element(By.XPATH, '//*[@id=\"myTable\"]')\nhasla = pd.read_html(table.get_attribute('outerHTML'))[0]\n\n# Flatten the MultiIndex columns\nhasla.columns = [' '.join(col).strip() if isinstance(col, tuple) else col for col in hasla.columns.values]\n\n# Rename 'Unnamed: 1_level_0 Team' to 'Team'\nhasla.rename(columns={'Unnamed: 1_level_0 Team': 'Team'}, inplace=True)\n\n# Extracting win/loss information and creating new columns\nhasla['Win'] = hasla['Team'].str.extract(r'\\((\\d+)-\\d+\\)')\nhasla['Loss'] = hasla['Team'].str.extract(r'\\(\\d+-(\\d+)\\)')\n\n# Remove parentheses and numbers from 'Team' column\nhasla['Team'] = hasla['Team'].replace(regex={'\\([^)]*\\)': '', '\\d+': ''})\n\nhasla['Team'] = hasla['Team'].str.strip()\n\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nhasla.to_excel(f'{desktop_path}/findhasla.xlsx', index=False)\n\n# Close the browser\ndriver.quit()\n\n\nScraping the second website teamrankings.com with BeautifulSoup\n\nCodedef scrape_and_merge(urls, new_column_names):\n    dfs = []\n\n    for url in urls:\n        response = requests.get(url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n        table = soup.find('table')\n        df = pd.read_html(StringIO(str(table)))[0]\n\n        if url in new_column_names:\n            df.columns = new_column_names[url]\n\n        for col in df.columns:\n            if col not in ['Rank', 'Team']:\n                df[col] = pd.to_numeric(df[col].replace('%', '', regex=True), errors='coerce')\n\n        dfs.append(df)\n\n    # Merge all DataFrames dynamically\n    combined_df = dfs[0]\n    for i, df in enumerate(dfs[1:], start=2):\n        combined_df = pd.merge(combined_df, df, on='Team', how='outer', suffixes=('', f'_{i}'))\n\n    # Drop duplicate 'Team' columns\n    combined_df = combined_df.loc[:, ~combined_df.columns.duplicated()]\n\n    return combined_df\n\n# Define the URLs\nurls = [\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate',\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct',\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game',\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession',\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession'\n]\n\n# Create a dictionary with new column names for certain URLs\nnew_column_names = {\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-free-throw-rate': ['Rank', 'Team', 'FTR_2023', 'FTR_L3', 'FTR_L1', 'FTR_Home', 'FTR_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/offensive-rebounding-pct': ['Rank', 'Team', 'ORB_2023', 'ORB_L3', 'ORB_L1', 'ORB_Home', 'ORB_Away', 'ORB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-offensive-rebounding-pct': ['Rank', 'Team', 'DRB_2023', 'DRP_L3', 'DRB_L1', 'DRB_Home', 'DRB_Away', 'DRB 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-three-point-pct': ['Rank', 'Team', 'opp3_2023', 'opp3_L3', 'opp3_L1', 'opp3_Home', 'opp3_Away', 'opp3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/three-point-pct': ['Rank', 'Team', 'p3_2023', 'p3_L3', 'p3_L1', 'p3_Home', 'p3_Away', 'p3 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-two-point-pct': ['Rank', 'Team', 'o2p_2023', 'o2p_L3', 'op2_L1', 'op2_Home', 'op2_Away', 'op2 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/two-point-pct': ['Rank', 'Team', '2p_2023', '2p_L3', '2p_L1', '2p_Home', '2p_Away', '2p 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/possessions-per-game': ['Rank', 'Team', 'Pace_2023', 'Pace_L3', 'Pace_L1', 'Pace_Home', 'Pace_Away', 'Pace 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/turnovers-per-possession': ['Rank', 'Team', 'TO_2023', 'TO_L3', 'TO_L1', 'TO_Home', 'TO_Away', 'TO 2022'],\n    'https://www.teamrankings.com/ncaa-basketball/stat/opponent-turnovers-per-possession': ['Rank', 'Team', 'oppTO_2023', 'oppTO_L3', 'oppTO_L1', 'oppTO_Home', 'oppTO_Away', 'oppTO 2022']\n}\n\n\nCombining the data frames and saving to desktop\n\nCode# Call the function to scrape and merge data\ncombined_df = scrape_and_merge(urls, new_column_names)\n\ncombined_df['Team'] = combined_df['Team'].replace({\n'Miami (OH)' : 'Miami'\n\n})\n# Save the DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\ncombined_df.to_excel(f'{desktop_path}/findme.xlsx', index=False)\n\n\n\nneutral_input = input(\"Is it a neutral site game (Yes/No): \")\n\n\n# Drop duplicate team names in hasla\nhasla = hasla.drop_duplicates(subset=['Team'])\n\n# Drop duplicate team names in combined_df\ncombined_df = combined_df.drop_duplicates(subset=['Team'])\n\n# Merge the DataFrames based on 'Team'\nmerged_df = pd.merge(hasla, combined_df, on='Team', how='inner')\n\n# Save the merged DataFrame to Excel\ndesktop_path = \"/Users/nathanbresette/Desktop\"\nmerged_df.to_excel(f'{desktop_path}/merged_data.xlsx', index=False)"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "href": "posts/NCAA Basketball/basketball.html#cleaning-data",
    "title": "NCAA Basketball Analysis",
    "section": "Cleaning Data",
    "text": "Cleaning Data\nAll further code performed in R:\nOnce the data has been combined into one data frame, there are over 100 variables. Using dplyr, 16 columns are selected, renamed for easier readability, mutated to correct variable type (numeric, factor, etc), and a new variable is feauture engineered to split the ranks into three categories of Ranked, Top 50%, and Bottom 50%.\n\nCodelibrary(readxl)\nlibrary(tidyverse)\nmerged_data &lt;- read_excel(\"~/Desktop/merged_data.xlsx\")\n\nclean_data &lt;- merged_data %&gt;%\n  select(`Unnamed: 0_level_0 Rk`, `Win`, `Loss`, `DEFENSIVE SUMMARY Eff`, `DEFENSIVE SUMMARY 3P%`, `DEFENSIVE SUMMARY FG%`, `DEFENSIVE SUMMARY MR%`, `DEFENSIVE SUMMARY NP%`, FTR_2023, TO_2023, ORB_2023, DRB_2023, p3_2023, `2p_2023`, Pace_2023, TO_2023) %&gt;%\n  rename(\n    Rank = `Unnamed: 0_level_0 Rk`,\n    `Def_Eff` = `DEFENSIVE SUMMARY Eff`,\n    `Def_3P` = `DEFENSIVE SUMMARY 3P%`,\n    `Def_FG` = `DEFENSIVE SUMMARY FG%`,\n    `Def_MR` = `DEFENSIVE SUMMARY MR%`,\n    `Def_NP` = `DEFENSIVE SUMMARY NP%`,\n    Off_FTR = FTR_2023,\n    Off_TO = TO_2023,\n    Off_ORB = ORB_2023,\n    Def_DRB = DRB_2023,\n    Off_3P = p3_2023,\n    Off_2P = `2p_2023`,\n    Pace = Pace_2023\n  ) %&gt;% \n  mutate(Win = as.numeric(Win),\n         Loss = as.numeric(Loss)) \n\nclean_data$Rank_Category &lt;- ifelse(clean_data$Rank &gt;= 0 & clean_data$Rank &lt;= 25, \"Ranked\",\n                                   ifelse(clean_data$Rank &gt; 25 & clean_data$Rank &lt;= 181, \"Top 50%\", \"Bottom 50%\"))\nclean_data &lt;- clean_data %&gt;%\n  mutate(Rank_Category = as.factor(Rank_Category))\n\n\nThe final data cleaning step is checking total NA values for each variable\n\nCodecbind(lapply(lapply(clean_data, is.na), sum))\n\n              [,1]\nRank          0   \nWin           0   \nLoss          0   \nDef_Eff       0   \nDef_3P        0   \nDef_FG        0   \nDef_MR        0   \nDef_NP        0   \nOff_FTR       0   \nOff_TO        0   \nOff_ORB       0   \nDef_DRB       0   \nOff_3P        0   \nOff_2P        0   \nPace          0   \nRank_Category 0"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "href": "posts/NCAA Basketball/basketball.html#exploratory-analysis",
    "title": "NCAA Basketball Analysis",
    "section": "Exploratory Analysis",
    "text": "Exploratory Analysis\nCorrelation and Scatter Plots\nThe data exploration begins by looking at the correlation between variables. I created a function to make a correlation plot then if the correlation is above the absolute value of 0.6, it will plot the scatter plot of the two correlated variables. Due to the high correlation in this data, I have limited the output to only two of the scatterplots.\n\nCodelibrary(corrplot)\n\ncompute_and_plot_correlation &lt;- function(data, threshold = 0.6) {\n  # Select numeric columns\n  numeric_data &lt;- data[, sapply(data, is.numeric)]\n  \n  # Remove rows with missing values\n  numeric_data &lt;- numeric_data[complete.cases(numeric_data), ]\n  \n  # Compute correlation matrix\n  correlation_matrix &lt;- cor(numeric_data)\n  \n  # Find pairs of variables with correlation above or below the threshold\n  high_correlation_pairs &lt;- which(abs(correlation_matrix) &gt; threshold & upper.tri(correlation_matrix), arr.ind = TRUE)\n  \n  # Create scatter plots for high correlation pairs\n  plots &lt;- list()\n  for (i in 1:nrow(high_correlation_pairs)) {\n    var_x &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 1]]\n    var_y &lt;- rownames(correlation_matrix)[high_correlation_pairs[i, 2]]\n    \n    plot &lt;- ggplot(data = numeric_data, aes_string(x = var_x, y = var_y)) +\n      geom_point() +\n      labs(title = paste(\"Scatter Plot of\", var_y, \"vs\", var_x), x = var_x, y = var_y) + \n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    plots[[paste(var_x, var_y, sep = \"_\")]] &lt;- plot\n  }\n  \n  # Plot correlation matrix\n  corrplot(correlation_matrix, method = \"shade\", type = \"lower\", diag = FALSE, addCoef.col = \"black\", number.cex = 0.5)\n  \n  return(plots)\n}\n\n#Example call to function\nscatter_plots &lt;- compute_and_plot_correlation(clean_data)\n\n\n\n\n\n\nCodefor (i in seq_along(scatter_plots)) {\n  if (i &gt; 2) break\n  print(scatter_plots[[i]])\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\nDistributions - Histograms\nI also made a function to make histograms for all numeric variables to view the distributions. Because all of our variables are numeric, no bar charts were made to view the distribution of categorical variables.\n\nCodecreate_histograms_ggplot &lt;- function(data) {\n  # Get numeric variable names\n  numeric_vars &lt;- names(data)[sapply(data, is.numeric)]\n  \n  # Initialize an empty list to store ggplot objects\n  plots &lt;- list()\n  \n  # Loop through each numeric variable and create a histogram using ggplot\n  for (var in numeric_vars) {\n    # Create ggplot object for histogram\n    plot &lt;- ggplot(data, aes_string(x = var)) +\n      geom_histogram(binwidth = 0.5, fill = \"skyblue\", color = \"black\") +\n      labs(title = paste(\"Histogram of\", var), x = var, y = \"Frequency\") +\n      theme_minimal() +\n      theme(plot.title = (element_text(hjust = 0.5)))\n\n    \n    # Append ggplot object to the list\n    plots[[var]] &lt;- plot\n  }\n  \n  return(plots)\n}\n\n# Example call to function\nhist_plots &lt;- create_histograms_ggplot(clean_data)\n\n\n  print(hist_plots[[2]])\n\n\n\n\n\n\nCode  print(hist_plots[[3]])"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "href": "posts/NCAA Basketball/basketball.html#principal-component-analysis-pca",
    "title": "NCAA Basketball Analysis",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\n\nCodelibrary(htmlwidgets)\nlibrary(plotly)\n\nX &lt;- subset(clean_data, select = -c(Rank_Category, Win, Loss))\n\nprin_comp &lt;- prcomp(X, center = TRUE, scale. = TRUE)\n\n\nScree Plot\n\nCodeplot(prin_comp, type = \"l\", main = \"Scree Plot\")\n\n\n\n\n\n\n\n3D PCA\n\nCodesumm &lt;- summary(prin_comp)\nsumm$importance[2,]\n\n    PC1     PC2     PC3     PC4     PC5     PC6     PC7     PC8     PC9    PC10 \n0.41190 0.13812 0.08788 0.06669 0.05603 0.05414 0.04966 0.04583 0.03675 0.03225 \n   PC11    PC12    PC13 \n0.01271 0.00427 0.00378 \n\nCodecomponents &lt;- prin_comp[[\"x\"]]\ncomponents &lt;- data.frame(components)\ncomponents$PC2 &lt;- -components$PC2\ncomponents$PC3 &lt;- -components$PC3\ncomponents = cbind(components, clean_data$Rank_Category)\n\n# Combine components with Ranked labels\ncomponents &lt;- cbind(components, Rank_Category = clean_data$Rank_Category)\n\n# Create Plotly figure\nfig &lt;- plot_ly(components, x = ~PC1, y = ~PC2, z = ~PC3, color = ~Rank_Category,\n               colors = c('#636EFA','#EF553B','#00CC96'), type = \"scatter3d\", mode = \"markers\",\n               marker = list(size = 4))\n\n\n# Customize layout\nfig &lt;- fig %&gt;% layout(\n  title = \"61.67% Variance Explained\",\n  scene = list(bgcolor = \"#e5ecf6\")\n)\n\n# Show the plot\nfig\n\n\n\n\nCodesaveWidget(fig, \"interactive_plot.html\")"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#neural-network",
    "href": "posts/NCAA Basketball/basketball.html#neural-network",
    "title": "NCAA Basketball Analysis",
    "section": "Neural Network",
    "text": "Neural Network\n\nCodelibrary(neuralnet)\nlibrary(caret)\nlibrary(tidymodels)\n\nnndata &lt;- clean_data \nset.seed(123)\n# Put 3/4 of the data into the training set \ndata_split &lt;- initial_split(nndata, prop = 3/4, strata = Rank_Category)\n\n# Create data frames for the two sets:\ntrain_data &lt;- training(data_split)\ntest_data  &lt;- testing(data_split)\n\n\nNN &lt;- neuralnet(Rank_Category ~ ., train_data, hidden = c(5,3), linear.output = TRUE)\nplot(NN, rep = \"best\")\n\n\n\n\n\n\n\nConfusion Matrix\n\nCodepredicted_classes &lt;- predict(NN, test_data)\n# Extract predicted class labels\npredicted_classes &lt;- max.col(predicted_classes)\n\n# Convert the indices to class labels\npredicted_classes &lt;- levels(test_data$Rank_Category)[predicted_classes]\nactual_classes &lt;- test_data$Rank_Category\n\npredicted_classes &lt;- factor(predicted_classes, levels = levels(actual_classes))\n\n# length(predicted_classes)\n# print(predicted_classes)\n\n# Extract actual class labels from the test data\n# length(actual_classes)\n# print(actual_classes)\n\n# Create a confusion matrix\nconfusionMatrix(predicted_classes, test_data$Rank_Category)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Bottom 50% Ranked Top 50%\n  Bottom 50%         42      0       0\n  Ranked              0      6       0\n  Top 50%             2      0      41\n\nOverall Statistics\n                                          \n               Accuracy : 0.978           \n                 95% CI : (0.9229, 0.9973)\n    No Information Rate : 0.4835          \n    P-Value [Acc &gt; NIR] : &lt; 2.2e-16       \n                                          \n                  Kappa : 0.9607          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Bottom 50% Class: Ranked Class: Top 50%\nSensitivity                     0.9545       1.00000         1.0000\nSpecificity                     1.0000       1.00000         0.9600\nPos Pred Value                  1.0000       1.00000         0.9535\nNeg Pred Value                  0.9592       1.00000         1.0000\nPrevalence                      0.4835       0.06593         0.4505\nDetection Rate                  0.4615       0.06593         0.4505\nDetection Prevalence            0.4615       0.06593         0.4725\nBalanced Accuracy               0.9773       1.00000         0.9800"
  },
  {
    "objectID": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "href": "posts/NCAA Basketball/basketball.html#xgboost-classification-model",
    "title": "NCAA Basketball Analysis",
    "section": "XGBoost Classification Model",
    "text": "XGBoost Classification Model\n\nCode#libs\nlibrary(janitor)\nlibrary(tidymodels)\nlibrary(caret)\nlibrary(pROC)\nlibrary(data.table)\nlibrary(kableExtra)\n\n\nSplitting into Training/Testing\n\nCodeDATA &lt;- clean_data %&gt;% \n  select(-Rank)\n\nset.seed(123)\nDATA_SPLIT &lt;- DATA %&gt;%\n  initial_split(strata = Rank_Category)\n\nDATA_TRAIN &lt;- training(DATA_SPLIT)\nDATA_TEST &lt;- testing(DATA_SPLIT)\n\nset.seed(234)\nDATA_folds &lt;- vfold_cv(DATA_TRAIN, strata = Rank_Category)\nDATA_folds\n\n#  10-fold cross-validation using stratification \n# A tibble: 10 × 2\n   splits           id    \n   &lt;list&gt;           &lt;chr&gt; \n 1 &lt;split [242/28]&gt; Fold01\n 2 &lt;split [242/28]&gt; Fold02\n 3 &lt;split [242/28]&gt; Fold03\n 4 &lt;split [242/28]&gt; Fold04\n 5 &lt;split [243/27]&gt; Fold05\n 6 &lt;split [243/27]&gt; Fold06\n 7 &lt;split [244/26]&gt; Fold07\n 8 &lt;split [244/26]&gt; Fold08\n 9 &lt;split [244/26]&gt; Fold09\n10 &lt;split [244/26]&gt; Fold10\n\n\nRecipe\n\nCodeDATA_rec &lt;-\n  recipe(Rank_Category ~ ., data = DATA_TRAIN) %&gt;%\n  step_unknown(all_nominal_predictors()) %&gt;%\n  step_dummy(all_nominal_predictors(), one_hot = TRUE)\n\nprep(DATA_rec) # checking prep\n\n\nTuning Model\n\nCodexgb_spec &lt;-\n  boost_tree(\n    trees = tune(),\n    min_n = tune(),\n    mtry = tune(),\n    tree_depth = tune(),\n    learn_rate = tune(),\n    loss_reduction = tune()) %&gt;%\n  set_engine(\"xgboost\") %&gt;%\n  set_mode(\"classification\")\n\n#workflow\nxgb_workfl &lt;- workflow(DATA_rec, xgb_spec)\n\n\nRace Anova\n\nCodelibrary(finetune)\ndoParallel::registerDoParallel()\n\nset.seed(345)\nxgb_rs &lt;- tune_race_anova(\n  xgb_workfl,\n  resamples = DATA_folds,\n  grid = 20,\n  metrics = metric_set(accuracy),\n  control = control_race(verbose_elim = TRUE)\n)\n\n\nComparing Models\n\nCodeanova &lt;- plot_race(xgb_rs)\n\nanova +\n  labs(title = \"Model Race ANOVA\",\n       y = \"Model Accuracy\") +\n  theme_minimal() +\n  theme(plot.title = (element_text(hjust = 0.5)))\n\n\n\n\n\n\n\nBest Model\n\nCodeshow_best(xgb_rs)\n\n# A tibble: 4 × 12\n   mtry trees min_n tree_depth learn_rate loss_reduction .metric  .estimator\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;      &lt;int&gt;      &lt;dbl&gt;          &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     \n1    12  1597    11          1     0.0191       2.36e- 1 accuracy multiclass\n2    14   908     8          6     0.0292       8.29e- 1 accuracy multiclass\n3     3   724    24         10     0.155        2.41e- 2 accuracy multiclass\n4     7   502     8          5     0.310        3.53e-10 accuracy multiclass\n# ℹ 4 more variables: mean &lt;dbl&gt;, n &lt;int&gt;, std_err &lt;dbl&gt;, .config &lt;chr&gt;\n\n\nMetrics\n\nCodexgb_last &lt;- xgb_workfl %&gt;%\n  finalize_workflow(select_best(xgb_rs, metric = \"accuracy\")) %&gt;%\n  last_fit(DATA_SPLIT)\n\nxgb_last$.metrics\n\n[[1]]\n# A tibble: 3 × 4\n  .metric     .estimator .estimate .config             \n  &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;               \n1 accuracy    multiclass     0.835 Preprocessor1_Model1\n2 roc_auc     hand_till      0.915 Preprocessor1_Model1\n3 brier_class multiclass     0.124 Preprocessor1_Model1\n\n\nConfusion Matrix\n\nCodeDATA_pred &lt;- collect_predictions(xgb_last)$.pred_class\n\nDATA_act &lt;- DATA_TEST$Rank_Category\n\nconfusionMatrix(DATA_pred, DATA_act)\n\nConfusion Matrix and Statistics\n\n            Reference\nPrediction   Bottom 50% Ranked Top 50%\n  Bottom 50%         37      0       4\n  Ranked              0      2       0\n  Top 50%             7      4      37\n\nOverall Statistics\n                                          \n               Accuracy : 0.8352          \n                 95% CI : (0.7427, 0.9047)\n    No Information Rate : 0.4835          \n    P-Value [Acc &gt; NIR] : 3.435e-12       \n                                          \n                  Kappa : 0.6965          \n                                          \n Mcnemar's Test P-Value : NA              \n\nStatistics by Class:\n\n                     Class: Bottom 50% Class: Ranked Class: Top 50%\nSensitivity                     0.8409       0.33333         0.9024\nSpecificity                     0.9149       1.00000         0.7800\nPos Pred Value                  0.9024       1.00000         0.7708\nNeg Pred Value                  0.8600       0.95506         0.9070\nPrevalence                      0.4835       0.06593         0.4505\nDetection Rate                  0.4066       0.02198         0.4066\nDetection Prevalence            0.4505       0.02198         0.5275\nBalanced Accuracy               0.8779       0.66667         0.8412\n\n\nVIP\n\nCodelibrary(vip)\nvip &lt;- extract_workflow(xgb_last) %&gt;%\n  extract_fit_parsnip() %&gt;%\n  vip(geom = \"col\", num_features = 10, mapping = aes(fill = Variable))\nvip"
  },
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "TidyTuesday: Retail Sales data analysis with Plotly in R\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nvisualization\n\n\nplotly\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\nNathan Bresette\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html",
    "href": "posts/Text Analysis/text_analysis.html",
    "title": "Text Classification and Sentiment Analysis",
    "section": "",
    "text": "Created text classification and sentiment analysis model to automate the process of classifying free response questions in a category then whether they were positive, neutral, negative, or a question."
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#abstract",
    "href": "posts/Text Analysis/text_analysis.html#abstract",
    "title": "Text Classification and Sentiment Analysis",
    "section": "Abstract",
    "text": "Abstract\nThis project was done during my intership at Lifeline Pregnancy. During the school year, they go to schools across Missouri and give a ‘Pure Freedom’ talk to the students. At the end of the talk, the students fill out a survey which has a free response question. The free response question is categorized into one of the four categories: Educator, Program, Content, or Other. The sentiment is then calculated as positive, negative, neutrall, or question. Before this model was created, all classification and sentiment was performed individually for every free response. Because the categories and sentiment are different than any pre-existing model, I had to make my own. To preserve the data and information of the clinic, Educators names have been replaced"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#data-cleaning",
    "href": "posts/Text Analysis/text_analysis.html#data-cleaning",
    "title": "Text Classification and Sentiment Analysis",
    "section": "Data Cleaning",
    "text": "Data Cleaning\nThe data cleaning process involves anticipating and correcting student typos, standardizing all words to lowercase, and eliminating pluralizations, with the function addressing some of these tasks.\n\nCodelibrary(tidyverse)\nlibrary(tidytext)\nlibrary(stringr)\nlibrary(caret)\n\ncom &lt;- read.csv(\"~/Desktop/comments - Sheet1.csv\", header=FALSE)\n\n#Different data set\n# com &lt;- read.csv(\"~/Desktop/PF Comment Coding  - Fall 2022-Spring 2023.csv\")\n# com &lt;- com %&gt;%\n#   rename(comments = Comment, labels = Aspect, sentiments = Quality)\n\n# Load necessary libraries\nlibrary(dplyr)\n\n# Filter and preprocess the data\npfcomments &lt;- com %&gt;%\n  filter(V2 != \"\") %&gt;%\n  rename(comments = V1, labels = V2, sentiments = V6) %&gt;% \n  mutate(comments = ifelse(comments == \"jakson\", \"Jackson\", comments)) %&gt;% \n  mutate(comments = ifelse(comments == \"under stode\", \"understood\", comments)) %&gt;% \n  mutate(comments = ifelse(comments == \"relation ships\", \"relationships\", comments)) %&gt;%\n  mutate(words_count = str_count(comments, \"\\\\S+\"))\n\n\nThe function is now created and preprocesses text data by converting it to lowercase, removing possessive apostrophes (’s), and then categorizes the comments based on the presence of certain keywords. If a comment contains only a smiley face “:)”, it is labeled as “Other”. Comments containing the word “sex” are labeled as “Remove”. Additionally, comments containing words like “like”, “liked”, “love”, or “loved” are also labeled as “Remove”. For other comments, it calculates the frequency of specific words related to educational programs, educators, and content, then assigns a label (“Educator”, “Program”, “Content”, or “Other”) based on the word frequencies and the length of the comment. The words that were removed are often split between categories so an individual still reviews them.\n\nCode# Define preprocess_and_extract_features function\npreprocess_and_extract_features &lt;- function(comment, label, words_count) {\n  # Convert text to lowercase\n  comment &lt;- tolower(comment)\n  comment &lt;- gsub(\"'s\", \"\", comment) \n\n  educator_words &lt;- c(\"jaclyn\",\"ryan\", \"ryan\", \"hanley\", \"hanley\",\"lynae\", \"lynae\",\"mcfarland\", \"mcfarland\", \"jackson\", \"jackson\", \"jacks\", \"page\", \"margo\", \"margo\",\"staiger\", \"amy\", \"posterick\", \"she\", \"he\", \"his\", \"her\",\"they\", \"guys\",\"man\", \"guy\",\"girl\", \"woman\", \"y’all\", \"yall\",  \"yourself\",\"bro\",\"good\", \"job\", \"good job\", \"did great\", \"great work\", \"great job\",\"speakers\", \"teacher\",\"teacher\", \"teachers\", \"teaching\",\"educator\", \"educators\", \"instructor\", \"instructor\", \"instructor\", \"instructors\",\"speaker\", \"awesome\", \"awsome\", \"fun\", \"sweet\", \"friendly\",\"personal\", \"story\", \"stories\", \"talk\")\n  program_words &lt;- c(\"learned\", \"sense\", \"it\", \"this\", \"sex\", \"sexual\", \"health\", \"thank\", \"thank\",\"thanks\", \"thanks\", \"thx\", \"program\", \"lesson\", \"lessons\",\"time\", \"good\", \"fun\", \"presentation\", \"know\", \"talk\", \"course\", \"educational\", \"class\", \"liked\")\n  content_words &lt;- c(\"relationship\", \"relation ships\",\"relationships\", \"std\", \"sti\", \"stds\", \"stis\", \"body\", \"anatomy\",\"porn\", \"pornography\", \"baby\", \"baby belly\", \"safe\",\"sexual\", \"butterfly effect\", \"pregnancy\", \"pregnancies\",\"pregnant\", \"boundary\", \"boundaries\", \"birth control\", \"condom\", \"plan b\",\"toxic\", \"puberty\", \"penis\", \"cock\", \"rape\", \"menstri\")\n\nprogram_count &lt;- sum(sapply(program_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\neducator_count &lt;- sum(sapply(educator_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\ncontent_count &lt;- sum(sapply(content_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n  \n  # If comment contains only \":)\", label as \"Other\"\n  if (grepl(\"^:\\\\)$\", comment)) {\n    pred_label &lt;- \"Other\"\n  }\n  # If comment contains \"sex\", label as \"Remove\"\n  else if (grepl(\"\\\\bsex\\\\b\", comment, ignore.case = TRUE)) {\n        pred_label &lt;- \"Remove\"\n  } \n  else if(grepl(\"\\\\blike\\\\b|\\\\bliked\\\\b|\\\\blove\\\\b|\\\\bloved\\\\b\", comment, ignore.case = TRUE))\n  {\n    pred_label &lt;- \"Remove\"\n  }\n  else {\n    program_count &lt;- sum(sapply(program_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n    educator_count &lt;- sum(sapply(educator_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n    content_count &lt;- sum(sapply(content_words, function(word) grepl(paste0(\"\\\\b\", word, \"\\\\b\"), comment, ignore.case = TRUE)))\n  \n    # If content_count is greater than or equal to 1, label as content\n    if (content_count &gt;= 1) {\n      pred_label &lt;- \"Content\"\n    } else {\n      # If educator_count is greater than program_count, label as educator\n      if (educator_count &gt; program_count) {\n        pred_label &lt;- \"Educator\"\n      } \n      # If program_count is greater than educator_count, label as program\n      else if (program_count &gt; educator_count) {\n        pred_label &lt;- \"Program\"\n      } \n      else if (program_count &gt;= 1 & educator_count &gt;= 1 & program_count == educator_count) {\n        pred_label &lt;- \"Remove\"\n      }\n      else if(words_count &lt; 35) {\n        pred_label &lt;- \"Remove\"\n      }\n      # If content_count is 0, educator_count or program_count is 1, do not label as other\n      else {\n        pred_label &lt;- \"Other\"\n      }\n    }\n  }\n  \n  return(data.frame(comments = comment, pred_label = pred_label, program_count, educator_count, content_count, labels = label))\n}\n\n\nConfusion Matrix\nThe function is now called and we split the data into correct and incorrect data sets. This allows us to see what content words did well and which ones did not. Additionally, a confusion matrix is used to show model accuracy where we specifically look at sensitivity and specificity\n\nCodeprocessed_data &lt;- Map(preprocess_and_extract_features, pfcomments$comments, pfcomments$labels, pfcomments$words_count) %&gt;%\n  bind_rows()\n\nprocessed &lt;-processed_data %&gt;% \n  filter(pred_label != \"Remove\")\n\nbad_processed_data &lt;- processed %&gt;% \n  filter(pred_label != labels) \n\n\nconfusionMatrix(factor(processed$labels), factor(processed$pred_label))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Content Educator Other Program\n  Content       55        2     0       8\n  Educator       4      410     0      29\n  Other          5        7    12       5\n  Program        8       27     1     301\n\nOverall Statistics\n                                          \n               Accuracy : 0.8902          \n                 95% CI : (0.8675, 0.9101)\n    No Information Rate : 0.5103          \n    P-Value [Acc &gt; NIR] : &lt; 2e-16         \n                                          \n                  Kappa : 0.8117          \n                                          \n Mcnemar's Test P-Value : 0.01733         \n\nStatistics by Class:\n\n                     Class: Content Class: Educator Class: Other Class: Program\nSensitivity                 0.76389          0.9193      0.92308         0.8776\nSpecificity                 0.98753          0.9229      0.98026         0.9322\nPos Pred Value              0.84615          0.9255      0.41379         0.8932\nNeg Pred Value              0.97899          0.9165      0.99882         0.9218\nPrevalence                  0.08238          0.5103      0.01487         0.3924\nDetection Rate              0.06293          0.4691      0.01373         0.3444\nDetection Prevalence        0.07437          0.5069      0.03318         0.3856\nBalanced Accuracy           0.87571          0.9211      0.95167         0.9049\n\n\nJustification for removing words over 35 based on the distributions of our correct and incorrect data sets. The means are much different too\n\nCodewordcount &lt;- pfcomments %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text, token = \"regex\") %&gt;% \n  group_by(comments) %&gt;%\n  summarise(words_count = n())  # Count words per comment\n\n#View(wordcloud)\n\nbad_wordcount &lt;- bad_processed_data %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text, token = \"regex\") %&gt;% \n  group_by(comments) %&gt;%\n  summarise(words_count = n())  # Count words per comment\n\nggplot(wordcount, aes(x = words_count)) +\n  geom_bar()\n\n\n\n\n\n\nCodeggplot(bad_wordcount, aes(x = words_count)) +\n  geom_bar()\n\n\n\n\n\n\nCodemean(wordcount$words_count)\n\n[1] 10.27065\n\nCodemean(bad_wordcount$words_count)\n\n[1] 12.5"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#most-common-words",
    "href": "posts/Text Analysis/text_analysis.html#most-common-words",
    "title": "Text Classification and Sentiment Analysis",
    "section": "Most Common Words",
    "text": "Most Common Words\n\nCodereplace_reg &lt;- \"https://t.co/[A-Za-z\\\\d]+|http://[A-Za-z\\\\d]+|&amp;|&lt;|&gt;|RT|https\"\nunnest_reg &lt;- \"([^A-Za-z_\\\\d#@']|'(?![A-Za-z_\\\\d#@]))\"\n\ntidycom &lt;- pfcomments %&gt;% \n  mutate(text = str_replace_all(comments, replace_reg, \"\")) %&gt;%\n  unnest_tokens(word, comments, token = \"regex\", pattern = unnest_reg) %&gt;% anti_join(stop_words)\n\n# tidycom %&gt;% count(word, sort = TRUE)\n\ntidycom %&gt;%\n  count(word, sort = TRUE) %&gt;%\n  filter(n &lt; 120) %&gt;%\n  filter(n &gt; 50) %&gt;%\n  mutate(word = reorder(word, n)) %&gt;%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#sentiment-analysis",
    "href": "posts/Text Analysis/text_analysis.html#sentiment-analysis",
    "title": "Text Classification and Sentiment Analysis",
    "section": "Sentiment Analysis",
    "text": "Sentiment Analysis\n\nCode# Apply sentiment analysis using the AFINN lexicon\nsentiment_scores &lt;- pfcomments %&gt;%\n  mutate(text = comments) %&gt;%\n  unnest_tokens(word, text, token = \"regex\") %&gt;%\n  \n  left_join(get_sentiments(\"afinn\"), by = \"word\") %&gt;%  # Use left_join to preserve all rows in pfcomments\n  group_by(comments) %&gt;%\n  summarise(sentiment_score = sum(value, na.rm = TRUE)) %&gt;%  # Handle NA values in sentiment scores\n  ungroup()\n\n# Classify sentiments\nsentiment_scores &lt;- sentiment_scores %&gt;%\n  mutate(sentiment = case_when(\n    sentiment_score &gt; 0 ~ \"Positive\",\n    sentiment_score &lt; 0 ~ \"Negative\",\n    TRUE ~ \"Neutral\"\n  ))\n\n# # Classify questions\n# question_words &lt;- c(\"what\", \"when\", \"where\", \"who\", \"whom\", \"which\", \"whose\", \"why\", \"how\")\n# sentiment_scores &lt;- sentiment_scores %&gt;%\n#   mutate(is_question = if_else(\n#     any(str_to_lower(comments) %in% question_words),\n#     \"Yes\",\n#     \"No\"\n#   ))\n# table(sentiment_scores$is_question)\n# Visualize sentiments and questions\nsentiment_scores %&gt;%\n  ggplot(aes(sentiment, fill = sentiment)) +\n  geom_bar() +\n  labs(title = \"Sentiment Analysis of Comments with Question Classification\")\n\n\n\n\n\n\n\nBar Chart of Sentiments\n\nCodepfcomments %&gt;%\n  ggplot(aes(sentiments, fill = sentiments)) +\n  geom_bar() +\n  labs(title = \"Sentiment Analysis of Comments\")"
  },
  {
    "objectID": "posts/Text Analysis/text_analysis.html#wordcloud",
    "href": "posts/Text Analysis/text_analysis.html#wordcloud",
    "title": "Text Classification and Sentiment Analysis",
    "section": "Wordcloud",
    "text": "Wordcloud\n\nCode#install.packages(\"wordcloud\")\nlibrary(wordcloud)\n\n#install.packages(\"RColorBrewer\")\nlibrary(RColorBrewer)\n\n#install.packages(\"wordcloud2\")\nlibrary(wordcloud2)\n\n#install.packages(\"tm\")\nlibrary(tm)\n#Create a vector containing only the text\ntext &lt;- pfcomments$comments\n# Create a corpus  \ndocs &lt;- Corpus(VectorSource(text))\n\ndtm &lt;- TermDocumentMatrix(docs) \nmatrix &lt;- as.matrix(dtm) \nwords &lt;- sort(rowSums(matrix),decreasing=TRUE) \ndf &lt;- data.frame(word = names(words),freq=words)\n\n\n\nCode#fc5f1b1b-2aeb-4e09-93fc-06fdac0d8030\n# Making DF for word clouds\n\n# Pre word cloud\ncorpus = Corpus(VectorSource(pfcomments$comments))\n\ncorpus &lt;- corpus %&gt;% \n  tm_map(removeNumbers) %&gt;%\n  tm_map(removePunctuation) %&gt;%\n  tm_map(stripWhitespace) %&gt;%\n  tm_map(content_transformer(tolower)) %&gt;%\n  tm_map(removeWords, stopwords(\"english\")) %&gt;%\n  tm_map(removeWords, stopwords(\"SMART\"))\n\ntdm = TermDocumentMatrix(corpus) %&gt;% \n  as.matrix()\n\nwords = sort(rowSums(tdm), decreasing = TRUE)\n\npre_WCdf = data.frame(words = names(words), freq = words)\n\n\n# Color Palettes\npre_WCcolors = c(\"#8bc13f\", \"#396430\", \"#6e6e6e\")\npre_WCbkgd = \"#FFFFFF\"\npost_WCcolors = c(\"#FFFFFF\", \"#510C76\", \"#87714D\")\npost_WCbkgd = \"#00A8E2\"\n\n#rm unneeded vars\nrm(corpus, tdm, words)\n\nWC_Pre &lt;- wordcloud2(pre_WCdf,\n           color = rep_len(pre_WCcolors, nrow(pre_WCdf)),\n           backgroundColor = pre_WCbkgd,\n           fontFamily = \"AppleMyungjo\",\n           size = .62,\n           rotateRatio = 0)\n\nWC_Pre"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "NCAA Basketball Analysis\n\n\n\n\n\n\nWebscraping\n\n\nPCA\n\n\nNeural Network\n\n\nXGBoost\n\n\nggplot\n\n\nplotly\n\n\n\n\n\n\n\n\n\nApr 12, 2024\n\n\nNathan Bresette\n\n\n\n\n\n\n\n\n\n\n\n\nText Classification and Sentiment Analysis\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nvisualization\n\n\nplotly\n\n\n\n\n\n\n\n\n\nJan 2, 2023\n\n\nNathan Bresette\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "aboutme_index.html",
    "href": "aboutme_index.html",
    "title": "Nathan Bresette",
    "section": "",
    "text": "I am a senior at Truman State University, majoring in Statistics with a concentration in Data Science and a minor in Mathematics. Passionate about uncovering insights from data, I am dedicated to leveraging statistical techniques and data analysis tools to solve real-world problems. With a strong foundation in mathematics and a keen interest in data science, I am enthusiastic about contributing to projects that drive innovation and make a positive impact.\n \nIn my portfolio, you’ll find a diverse range of projects, including analyses, visualizations, and applications that showcase my skills and experiences. From developing R packages to conducting consulting projects and participating in DataFest competitions, I’ve gained practical knowledge and hands-on experience in various aspects of data science. I invite you to explore my portfolio to learn more about my work and how I can contribute to your projects and initiatives."
  }
]